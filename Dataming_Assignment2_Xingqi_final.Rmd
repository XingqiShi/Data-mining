---
title: "Data_mining_Assignment2_Xingqi"
author: "Xingqi"
date: "February 4, 2015"
output: pdf_document
---

# Part 1
Use the GermanCredit data (package caret) in R or from UCI Machine Learningâ€™s website.
Latent Class Analysis Homework
1.	Perform latent class analysis of only the categorical variables for market segmentation using (function poLCA in package poLCA). Remember: the local optima problem is big for all the clustering and latent class methods.
2.	Select the variables that you think are appropriate and useful.
3.	Determine 2, 3,..,K class/cluster solutions. Remember to run from multiple random starts. Use AIC and BIC criteria and interpretation based on graphs to interpret LCA solutions.
4.	Perform Holdout validation of LCA.
a.	For Holdout, use the centers class-conditional probabilities - probs - from training set as input to probs.start for holdout (generated from the training set LCA solution, as the starting point for holdout. Use similarity of relative class sizes and hold-out class conditional probabilities as measures of stability.
5.	Provide implications / commentary on the goodness, interpretability, stability, and adequacy of solutions. 
6.	Comment on the similarity/differences between the clustering solutions you generated in Assignment 1 with the solution you generated using LCA.

##1.  Perform latent class analysis of only the categorical variables for market segmentation using (function poLCA in package poLCA). Remember: the local optima problem is big for all the clustering and latent class methods.

load the data.


```{r message=FALSE, warning=FALSE}
library(caret)
library(poLCA)
data(GermanCredit)
#str(GermanCredit)
dim(GermanCredit)
GermanCredit$Class<-as.numeric(GermanCredit$Class)
```

poLCA only work for non-zero dataset.I add one to all the values in the dataset.

```{r}
German.LCA<-GermanCredit[,]+1
```

Pick out some credit related variables are potentailly important to credit classification and feed these variables into 3 clusters latent classification.


```{r}
f1<-cbind(
        CheckingAccountStatus.lt.0,
        CheckingAccountStatus.0.to.200,
        CheckingAccountStatus.gt.200,
        CheckingAccountStatus.none,
        CreditHistory.NoCredit.AllPaid,
        CreditHistory.ThisBank.AllPaid,
        CreditHistory.PaidDuly,
        CreditHistory.Delay,
        CreditHistory.Critical,
        SavingsAccountBonds.lt.100,
        SavingsAccountBonds.100.to.500,
        SavingsAccountBonds.gt.1000,
        SavingsAccountBonds.Unknown,
        EmploymentDuration.lt.1,
        EmploymentDuration.1.to.4,
        EmploymentDuration.4.to.7,
        EmploymentDuration.Unemployed,
        Housing.Rent,
        Housing.Own,
        Housing.ForFree)~Class
results.2=poLCA(f1,German.LCA,nclass=3,nrep=10,tol=.001,verbose=FALSE, graphs=TRUE)
results.3=poLCA(f1,German.LCA,nclass=3,nrep=10,tol=.001,verbose=FALSE, graphs=TRUE)
results.4=poLCA(f1,German.LCA,nclass=3,nrep=10,tol=.001,verbose=FALSE, graphs=TRUE)
results.5=poLCA(f1,German.LCA,nclass=3,nrep=10,tol=.001,verbose=FALSE, graphs=TRUE)
```


Check the output of poLCA.

```{r}
attributes(results.2)
results.2$npar
table(results.2$predclass)
c(results.2$aic,results.3$aic,results.4$aic)
```


## 2.	Select the variables that you think are appropriate and useful.

From output of results.2 using poLCA function, I would choose the variables show distinct response probabilities between different clusters. Five variables are retained for further analysis CheckingAccountStatus.lt.0, CheckingAccountStatus.0.to.200, CheckingAccountStatus.none, CreditHistory.NoCredit.AllPaid, CreditHistory.Critical. These variables also contain important credit information of the customers. 

The following LCA is based on four chosen variables.

```{r}
f1a<-cbind(
        CheckingAccountStatus.lt.0, 
        CheckingAccountStatus.0.to.200, 
        CheckingAccountStatus.none, 
        CreditHistory.NoCredit.AllPaid, 
        CreditHistory.Critical)~1
resultsa.2=poLCA(f1a,German.LCA,nclass=3,nrep=10,tol=.001,verbose=FALSE, graphs=TRUE)
resultsa.3=poLCA(f1a,German.LCA,nclass=3,nrep=10,tol=.001,verbose=FALSE, graphs=TRUE)
resultsa.4=poLCA(f1a,German.LCA,nclass=3,nrep=10,tol=.001,verbose=FALSE, graphs=TRUE)
resultsa.5=poLCA(f1a,German.LCA,nclass=3,nrep=10,tol=.001,verbose=FALSE, graphs=TRUE)
resultsa.2$aic
resultsa.2$bic
```


## 3.	Determine 2, 3,..,K class/cluster solutions. Remember to run from multiple random starts. Use AIC and BIC criteria and interpretation based on graphs to interpret LCA solutions.

```{r}

resultsa<-c()
AIC<-c()
BIC<-c()
for (i in 2:10) {
        resultsa[[i]]<-poLCA(f1a,German.LCA,nclass=i,nrep=10,tol=.001,verbose=FALSE, graphs=TRUE)
        AIC[i]<-(resultsa[[i]])$aic
        BIC[i]<-(resultsa[[i]])$bic
                }
rbind(AIC,BIC)      
```

The AIC and BIC value is lowest when I use nclass=3. From the cluster sizes with different classification, the cluster sizes are relatively even distributed and the distribution of different variable are distinct between groups. 

## 4.	Perform Holdout validation of LCA.
a.	For Holdout, use the centers class-conditional probabilities - probs - from training set as input to probs.start for holdout (generated from the training set LCA solution, as the starting point for holdout. Use similarity of relative class sizes and hold-out class conditional probabilities as measures of stability.

Split samples into two parts. 60% of samples use for training model while 40% of samples use for validation.

```{r}
length <- nrow(German.LCA)
L<-1:length
training<-sample(L,0.6*length,replace=FALSE)
trainingdata<- German.LCA[training,]
holdout<- German.LCA[-training,]
```

Perform latent classification on training data.

```{r}
results.train<-c()
AIC.train<-c()
BIC.train<-c()
for (i in 2:6) {
        results.train[[i]]<-poLCA(f1a,trainingdata,nclass=i,nrep=10,tol=.001,verbose=FALSE, graphs=TRUE)
        AIC.train[i]<-(results.train[[i]])$aic
        BIC.train[i]<-(results.train[[i]])$bic
                }
 rbind(AIC.train,BIC.train)   
```

The AIC and BIC value is lowest when I use nclass=3. Next, I checked few more runs with nclass=3.


```{r}
results.train3.1<-poLCA(f1a,trainingdata,nclass=3,nrep=10,tol=.001,verbose=FALSE, graphs=TRUE)
results.train3.2<-poLCA(f1a,trainingdata,nclass=3,nrep=10,tol=.001,verbose=FALSE, graphs=TRUE)
results.train3.3<-poLCA(f1a,trainingdata,nclass=3,nrep=10,tol=.001,verbose=FALSE, graphs=TRUE)
```

From the cluster sizes of results.train3.1, the cluster sizes are relatively even distributed and the distribution of different variables are distinct between groups. 

apply the setting to hold out samples.

```{r}
results.train3.1
probs.start.train <- results.train3.1$probs.start
lc <- poLCA(f1a, holdout, nclass = 3, probs.start = probs.start.train)
```

The population shares of validation dataset are similar to the training dataset. The G^2(3) and X^2(3) of validation dataset are close to the values in training dataset. The response probabilities of different variables in different classes are similar between training dataset and validation dataset. The model predict in validation dataset are behaviour well.

# 5.	Provide implications / commentary on the goodness, interpretability, stability, and adequacy of solutions. 

In this analysis, I used the latent classification to determine which variables are potentially important to Credit class(good or bad). I choose five variables in the further analysis. Different class solutions were performed. I found nclass=3 has lowest AIC and BIC and sizes of cluster are evenly distributed. Further I performed the nclass=3 latent classification on training dataset. Using the probs.start from training dataset,  similar classifcation results were observed in the validation dataset. These demonstrate that the model is stable. Using this classification, CheckingAccountStatus.none is associated with good CreditHistory.Critical while CheckingAccountStatus.lt.0 is associated with bad CreditHistory.Critical. This make sense to certain extend. 


# 6.	Comment on the similarity/differences between the clustering solutions you generated in Assignment 1 with the solution you generated using LCA.

In assignment 1, I used kmeans solutions and komean solutions to cluster the dataset. In this assignment, I used latent classification to cluster the dataset. Kmeans solutions or komeans solutions are used to numeric data while latent classification solutions are used to catergorical data. kmeans solutions or komeans solutions are classified by the means of the clusters while latent classification are classified by dependence between variables. Both solutions interpret the credit information fine if I can find good variables to the model.


#Part 2

# 1.        Split sample into two random samples of sizes 60%  and 40%.

```{r}
length <- nrow(GermanCredit)
L<-1:length
training<-sample(L,0.6*length,replace=FALSE)
GermanCredit.pca<- scale(GermanCredit)
train.pca<- GermanCredit.pca[training,]
holdout.pca<- GermanCredit.pca[-training,]
```

##2.	Perform principal components of variables 1:7 from the GermanCredit Data on training sample. 

```{r}
pca<-prcomp(train.pca[,1:7],retx=TRUE,tol=0.4)
summary(pca)
```



##3.	Generate Scree Plots and select number of components you would retain.

```{r}
Variance<-summary(pca)$importance[2,];Variance
barplot(Variance,width=2, col="black", 
        names.arg=c("PC1","PC2","PC3","PC4","PC5","PC6","PC7"),
        ylim=c(0,1))
```


```{r}
screeplot(pca,type="line")
```

Based on the screeplot, the variances drop slow after PC4. I would choose 4 principle components into my further analysis. The first 4 principal components explain 73.59% of total variance.

##4.	Plot Component 1 loadings versus Component 2 loadings. Use this plot to interpret and name the Components. 

```{r}

matplot(pca$rotation[,1:2],type='l',lty=c(1,1),lwd=c(3,3),ylab="Loadings")
legend("topright",legend=c("PC1","PC2"),
       lty=c(1,1),
       lwd=c(3,3),
       col=c("black","red"))
```

The first two loadings in PC1 are above 0.6 and rest loading in PC1 are close to 0. I would name PC1 as Savings. The loadings of PC2 are below 0.  The most negative loadings in PC2 are Age and Residence Duration. I would name PC2 as custermer.information.


##5.	Repeat step (3) by plotting Component (1) separately versus all components you decided to retain from Step 3 (Component 3, Component 4 etc).



The plot of loadings in PC1 is as below.
```{r}
plot(pca$rotation[,1],type='l',lty=1,lwd=3,ylab="Loadings")
legend("topright",legend="PC1",
       lty=1,lwd=3,
       col="black")
```

The plot of loading in PC1, PC2, PC3 and PC4 is as below.

```{r}
matplot((pca$rotation[,1])/(pca$rotation[,1:4]),type='l',lty=1,
        lwd=3,
        col=c("black","red","green","blue"),
        ylab="Loadings",ylim=c(-1,1),xlim=c(0.5,8.5))
legend("topright",legend=c("PC1","PC2","PC3","PC4"),
       lty=1,
       lwd=3,
       col=c("black","red","green","blue"))

```

I would name PC3 as Installment and PC4 as CreditNumber.

6.	Make Sure to interpret each of the components you decide to retain. In case a component is not interpretable, note that.



7.	Show that Component loadings are orthogonal.

To show the component loadings are orthogonal, the dot product of loadings are calculated as below.

```{r}
library(mosaic)
loadings<-pca$rotation
loading.dot <- matrix(data=NA,nrow=7,ncol=7)
for (i in 1:7) {
        for (j in 1:7)
        loading.dot[i,j]=dot(loadings[,i],loadings[,j])
        }
loading.dot
round(loading.dot,10)
```

From above calculation, the dot products between different loadings of principle components are all 0. So the Component loadings are orthogonal.


8.	Show that Component scores are orthogonal.

To show the component scores are orthogonal, the dot product of scores are calculated as below.


```{r}
library(mosaic)
scores<-pca$x
dotall <- matrix(data=NA,nrow=7,ncol=7)
for (i in 1:7) {
        for (j in 1:7)
        dotall[i,j]=dot(scores[,i],scores[,j])
        }
dotall
round(dotall,10)
```

From above calculation, the dot products between different scores of principle components are all 0. So the Component scores are orthogonal.

9.	Perform Holdout validation of Principal Components solution.
a.	For Holdout validation, you will have to  
i.	predict the component scores in the Holdout  using the predict() function and 
ii.	matrix multiply the predicted component scores from (i) above with transpose of component loadings you derived from training data set from Step 2 above.  Refer to Page 52 of Class Lecture for Session 4 for details.  Understand the R commands on that slide.

The prediction of component scores in holdout is as below.

```{r}
round(cor(as.vector(train.pca[,1:7]), as.vector(pca$x %*% t(pca$rotation))),2)
y= predict(pca, newdata=holdout.pca[,1:7])
dim(y)
```

y is the predict scores in holdout. It contains 7 scores of principal component. The original holdout dataset is y multiply transpose of component loadings (including all 7 principal components). The calculation can be express as below.

```{r}
round(cor(as.vector(holdout.pca[,1:7]), as.vector(y %*% t(pca$rotation))),2)
```

The correlation coefficiency is 1. These result suggest that the original holdout dataset is equal to y multiply transpose of component loadings (including all 7 principal components).

The following calculation is based on the first four components only. First the pca including only first 4 principle component is built as below.

```{r}
pca2<-prcomp(train.pca[,1:7],retx=TRUE,tol=0.70)
pca2$rotation
round(cor(as.vector(train.pca[,1:7]), as.vector(pca2$x %*% t(pca2$rotation))),2)
```

The correlation coefficiency between train dataset and scores multiply transpose of component loadings (including 4 principal components) is 0.86. 


```{r}
y2= predict(pca2, newdata=holdout.pca[,1:7])
round(cor(as.vector(holdout.pca[,1:7]), as.vector(y2 %*% t(pca2$rotation))),2)
```

The correlation coefficiency between holdout dataset and scores of holdout dataset multiply transpose of component loadings (including 4 principal components) is 0.85. This value is very close to the value in taining dataset.


10.	Compute the Variance Account For (R2) in the Holdout sample. That yields a measure of Holdout performance. 

The explained cumulated variance explained in pca is

```{r}
summary(pca)
```

It can be calculated as the variance of scores in all components divided by the variance of scores in the first 4 components.


```{r}
sum(apply(pca2$x,2,var))/sum(apply(pca$x,2,var))
```

In same way, the Variance Account For (R2) in the Holdout sample can be calculated as below.


```{r}
sum(apply(y2,2,var))/sum(apply(y,2,var))
```

The R2 in holdout sample is 0.7251104, which is very close to R2 in train sample.

11.	Rotate the component loadings using varimax rotation. Use R function varimax() for it.  Look at the Loadings from the varimax rotation. Does it yield any different Interpretation of the Principal Components? 

The rotated loading of varimax is as below.

```{r}
rotated.loading <- varimax(pca$rotation)$rotmat
```

The matrix transformation is as below.

```{r}
round(cor(as.vector(train.pca[,1:7]), as.vector((pca$x %*% rotated.loading) %*% t(pca$rotation %*% rotated.loading))),2)
```

The orginal train matrix is equal to (scores * rotated.loading) * transpose (loadings*rotated.loading).

12.	Plot rotated loadings(1) versus rotated loadings (2) and (3).

The rotated loadings 1:3 are plotted as below.

```{r}
matplot(rotated.loading[,1]/rotated.loading[,1:3],type='l',lty=1,
        lwd=3,
        col=c("black","red","green"),
        ylab="Loadings",ylim=c(-1,1),xlim=c(0.5,8.5))
legend("topright",legend=c("PC1","PC2","PC3"),
       lty=1,
       lwd=3,
       col=c("black","red","green"))

```




13.	Do you think Principal Components reduced this data a lot? Do you like the solution? 

Principal Components reduced this dataset into 4 principle component.  I started with only 7 variables in the dataset and reduced to 4 principal components to cover 73% of total variance. Using the 4 principal components, I can explain more than 72.5% variance in holdout samples.  PCA analysis not only reduces variables in dataset, but also reduces the correlation between differnt variable into orthoganal principle components. PCA analysis is a great tool to large scaled data analysis.

