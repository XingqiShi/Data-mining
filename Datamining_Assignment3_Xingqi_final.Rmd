---
title: "Datamining_Assignment3_Xingqi"
author: "Xingqi"
date: "February 18, 2015"
output: html_document
---

##Part 1

Use the GermanCredit data (package caret) in R
Logistic Regression

Load the dataset

```{r message=FALSE, warning=FALSE}
library(caret)
library(MASS)
data(GermanCredit)
#str(GermanCredit)
dim(GermanCredit)
```

Logistic regression model will be used in this analysis. To be easier to understand output, the Credit "Class" is converted to numeric as below. After convertion, 0 stands for bad class and 1 stands for good class.

```{r}
GermanCredit$Class<-as.numeric(GermanCredit$Class)-1
```


###1.        Generate training and holdout samples for the GermanCredit data set, of sizes 700 and 300, using the sample() function. Call them, say, Train and Holdout. SAVE Train and Holdout. You will need these samples for comparison for other models you will build in later classes.

The dataset is split into two parts based on the Credit Class as below.

```{r}
good.class <- GermanCredit[GermanCredit$Class==1,]
bad.class <- GermanCredit[GermanCredit$Class==0,]
```

70% of samples are taken from good.class and bad.class for train dataset.
The remaining 30% of samples are kept in a holdout dataset.

```{r}
L1 <- nrow(good.class)
set.seed(12345)
sample.good <-sample(1:L1,round(L1*0.7,1),replace=FALSE)
train.good <- good.class[sample.good,]
holdout.good <- good.class[-sample.good,]
```



```{r}
set.seed(23456)
sample.bad <- sample(1:nrow(bad.class),round(nrow(bad.class)*0.7,1),replace=FALSE)
train.bad <- bad.class[sample.bad,]
holdout.bad <- bad.class[-sample.bad,]
```

Combine the training data from good.class and bad.class into one file named as train. In the same way, holdout dataset is combined together.

```{r}
train<-rbind(train.good,train.bad)
holdout<-rbind(holdout.good,holdout.bad)
dim(train)
dim(holdout)
```


###2.	Build a Logistic Regression model for Train for predicting the “Class” variable, using the other variables. Try to build the model with the lowest “AIC” that you can achieve. Use function glm() in R for this. Examples of the R scripts for glm() are available in the Class Lecture or the R scripts file for Session 5 on Chalk. 

A logistic regression model for train data is generated as below using Class as output and other variables as input. The 

```{r}
glm.model.full<-glm(Class~., data=train,family=binomial(link=logit))
summary(glm.model.full)
```

The model explain (855.21-573.76)/855.21=32.91% of total deviance.

Different links of logistic regression are tried as follow. 

```{r}
glm.model.probit<-glm(Class~., data=train,family=binomial(link=probit))
glm.model.c<-glm(Class~., data=train,family=binomial(link=cloglog))
```

The AIC values of models with different links are summarized as below.

```{r}
c(glm.model.full$aic,glm.model.probit$aic,glm.model.c$aic)
```

The model with logit link has the lowest AIC value.

Next, models are generated with fewer variables as below. 

```{r}
glm.model2<-glm(Class~Duration+Amount+InstallmentRatePercentage, data=train,family=binomial(link=logit))
glm.model2$aic
```

The AIC value of above glm.model2 is much higher than the full model.

Using the summary of glm.model.full, all statistically significant variables are selected as input variables in the following model.

```{r}
glm.model.sig<-glm(Class~(Duration+Amount+InstallmentRatePercentage+Age+CheckingAccountStatus.lt.0+CheckingAccountStatus.0.to.200+CreditHistory.NoCredit.AllPaid+CreditHistory.ThisBank.AllPaid+CreditHistory.PaidDuly+CreditHistory.Delay+Purpose.NewCar+Purpose.Furniture.Equipment+Purpose.Radio.Television+Purpose.DomesticAppliance+Purpose.Repairs+Purpose.Education+Purpose.Business+SavingsAccountBonds.lt.100+EmploymentDuration.4.to.7+Property.RealEstate+Property.Insurance), 
                   data=train,family=binomial(link=logit))
glm.model.sig$aic
```

The AIC value is lower than the AIC value in glm.model.full. 


Take the statistically significant (at 0.001 level) variables in glm.model.sig and generated a model as below.

```{r}
glm.model.sig.high<-glm(Class~(Duration+Amount+InstallmentRatePercentage+Age+CheckingAccountStatus.lt.0+CheckingAccountStatus.0.to.200+CreditHistory.NoCredit.AllPaid+CreditHistory.ThisBank.AllPaid+CreditHistory.PaidDuly+Purpose.NewCar+Purpose.Furniture.Equipment+Purpose.DomesticAppliance+Purpose.Education+Purpose.Business+SavingsAccountBonds.lt.100+EmploymentDuration.4.to.7), data=train,family=binomial(link=logit))
glm.model.sig.high$aic
```


The AIC of this model is higher than the model containing only main effects and is lower than the full model.



###3.	Choose only the “main-effects”. Main-effects are simply the original variables “as is” (of course, you need to ensure that categorical variables are represented with (k-1) dummy variables that have values 0 or 1. Function glm()  converts all factor variables automatically into dummies. You don’t have to manually form dummy variables for factor variables.

Using the summary of glm.model.full, the "main-effects" (statistically significant) variables are selected as input variables in the following model (also show in last part).


```{r}

glm.model.sig<-glm(Class~(Duration+Amount+InstallmentRatePercentage+Age+CheckingAccountStatus.lt.0+CheckingAccountStatus.0.to.200+CreditHistory.NoCredit.AllPaid+CreditHistory.ThisBank.AllPaid+CreditHistory.PaidDuly+CreditHistory.Delay+Purpose.NewCar+Purpose.Furniture.Equipment+Purpose.Radio.Television+Purpose.DomesticAppliance+Purpose.Repairs+Purpose.Education+Purpose.Business+SavingsAccountBonds.lt.100+EmploymentDuration.4.to.7+Property.RealEstate+Property.Insurance), data=train,family=binomial(link=logit))
summary(glm.model.sig)
```

The AIC value is lower than the full model.


###4.	Try various independent variable combinations until you achieve the lowest AIC.  Choose this as the best model, and present its summary using the summary() function in R. You don’t have to use any multiplicative interactions or non-linear transformations of the variables for this exercise.  Save this lowest AIC solution.

The stepAIC function in MASS is used to search for the lowest AIC model with the combination of variables. 

```{r}
glm.step<-stepAIC(glm.model.full,trace=FALSE)
summary(glm.step)
```

From summary of glm.step, the final model contains 30 variables as independent variables.


The "main effects" variables in glm.step are used to generate a model as below.

```{r}
glm.step.sig<-glm(formula = Class ~ Duration + Amount + InstallmentRatePercentage + Age + CheckingAccountStatus.lt.0 + CheckingAccountStatus.0.to.200 + CreditHistory.NoCredit.AllPaid + CreditHistory.ThisBank.AllPaid + CreditHistory.PaidDuly + CreditHistory.Delay + Purpose.NewCar + Purpose.Furniture.Equipment + Purpose.Radio.Television + Purpose.DomesticAppliance + Purpose.Repairs + Purpose.Education + Purpose.Business + SavingsAccountBonds.lt.100 + SavingsAccountBonds.100.to.500 + EmploymentDuration.4.to.7 + Property.RealEstate + Property.Insurance, family = binomial(link = logit), 
    data = train)

glm.step.sig$aic
```

The AIC value of glm.step.sig is a little higher than glm.step model. So glm.step model in this part is the model with lowest AIC values. I would save this model for further analysis.


###5.	Generate the confusion matrix  (counts and proportions of actual “Bad” and actual “Good” versus predicted “Bad” and predicted “Good”) using the lowest AIC model. Do you like the model? Why or why not.

The confusion matrix is as below.

```{r}
fitted<-glm.step$fitted.values
fitted[fitted>=0.5]=1
fitted[fitted<0.5]=0
table1<-table(actual.Class=train$Class,prediction=fitted);table1
```

Using the glm.step model in train dataset, 444 Good Class is correctly predicted and 117 bad Class is correctly predicted. The overall accuracy is (117+444)/(117+93+46+444) = 80.14%. The misclassification rate is (93+46)/(117+93+46+444) = 19.86%.


```{r}
round(prop.table(table1,1),2)
```

In train dataset, the glm.step model predicts 91% of actual good credit class and 56% of actual bad credit class. The true positive rate is 91% and false postive rate is 44%. The prediction performance of this method is good for good credit class. The prediction of bad credit class is not as good as good credit class. 

```{r}
round(prop.table(table1,2),2)
```

In fitted values, 83% of prediction is correct in good credit class and 72% of prediction is correct in bad credit class.

###6.	Perform Holdout validation testing.
###a.	Generate the Confusion matrix in the holdout sample. Do you like the results? Why or why not?

The prediction using glm.step on holdout sample is performed as below.

```{r}
xp=predict(glm.step, newdata=holdout,type="response")
xp[xp>=0.5]=1
xp[xp<0.5]=0
glm.step.prediction<-as.data.frame(c(fitted,xp))
write.csv(glm.step.prediction,file="glm.step.prediction")
prop.table(table(actual.class=GermanCredit$Class,glm.step.prediction[,2]))
```

The below table and prob.table show the comparision of actual credit class and the prediction in holdout.

```{r}
table2<-table(actual.class=holdout$Class,prediction=xp);table2
round(prop.table(table2,1),2)
round(prop.table(table2,2),2)
```

The overall accuracy is (40+182)/(40+50+28+182) = 74%. The misclassification rate is (50+28)/(40+50+28+182) = 26%. These values are close to the overall accuracy 80.14% and misclassification rate 19.86% in train data. This means the predict performance of this model is stable.

In holdout dataset, the glm.step model predict 87% of actual good credit class and 44% of actual bad credit class. The true positive rate is 87% and false postive rate is 56%. The predict performance of the model is similar in train data and holdout.  

In prediction, 78% of prediction is correct in good credit class and 59% of prediction is correct in bad credit class.

I like this result. The prediction performance of this model in train dataset is similar to the performance in holdout dataset. The overall accuracy is 74% in train dataset and 80.14% in holdout. This model performance is good and stable. So I like this model in term of performance. 30 variables are used as independent variables in the model. This model is a relative complicated model.

###7.	Summarize your results.

The logistic regression models are used in this part of analysis. The dataset was split into two datasets, train and holdout. Using credit Class as output and all others as independent variables in train dataset, a full model is generated in the first step. The summary of glm.model.full is interpreted. Different models with different combination independent variables were further generated. A glm.step model with the lowest AIC value is selected as a final model. The output variable in glm.step is Credit Class and independent variables are 30 variables from the original dataset. Using glm.step model, the overall accuracy is 80.14% and misclassification rate is 19.86% in train dataset. In validation dataset, the overall accuracy is 74% and the misclassification rate is 26%. So the model performance in train dataset and holdout dataset is good and stable.

##Part2
Tree Models
Use  the same the training and holdout samples for the GermanCredit data set that you used for Logistic Regression Assignment.

###1.        Build a Classification Tree model for the Training data set for predicting the “Class” variable, using the other variables.  Fist, grow a large tree by fixing cost complexity parameter = 0, and choose a minimum node size for splitting as 30. Use a 10 fold cross-validation. Examples of the R scripts for rpart() are available in the Class Lecture or the R scripts file for Session 6 on Chalk. 

A classification tree model for train data is generated using credit Class as output and other variables as independent variables.

```{r}
library(rpart)
set.seed(123)
x=rpart(Class~.,data=train,method="class",control=rpart.control(cp=0,minsplit=30,xval=10, maxsurrogate=0))
par(mai=c(0.1,0.1,0.1,0.1))
plot(x,main="Complete Tree: train",col=3, compress=TRUE, branch=0.2,uniform=TRUE)
text(x,cex=0.6,col=4,use.n=TRUE,fancy=FALSE,fwidth=0.3,fheight=0.3,bg=c(5))

```

From the above plot, there are 17 interactions.

###2.	Evaluate the complexity parameter plots and prints – using printcp() and plotcp() functions. Choose the cp value corresponding to lowest cross-validation error (xerror), and build the reduced size (pruned) tree in the training data set using the cp value corresponding to the lowest xerror.

The output of printcp is as below.

```{r}
printcp(x)
which.min(x$cptable[,"xerror"])
```

11 independent variables are used in this tree regression. The root node error is 0.3. cp values, nsplit, rel.error, xerror,xstd are also summarized in printcp. The lowest xerror is at 2 splits.

```{r}
plotcp(x,minline=TRUE,col=4)
```

The lowest xerror is at 2 splits. cp value corresponding to the lowest cross-validation error is chosen to build up a pruned tree.

```{r}
min.cp<-x$cptable[which.min(x$cptable[,"xerror"]),"CP"]
set.seed(123)
x2=rpart(Class~.,data=train,method="class",control=rpart.control(cp=min.cp,minsplit=30,xval=10, maxsurrogate=0))
par(mai=c(0.1,0.1,0.1,0.1))
plot(x2,main="Classification Pruned Tree: train",col=3, compress=TRUE, branch=0.2,uniform=TRUE)
text(x2,cex=0.6,col=4,use.n=TRUE,fancy=TRUE,fwidth=0.4,fheight=0.4,bg=c(5))
```

There are only 4 interactions in this pruned tree.

The tree regression is as below.

```{r}
printcp(x2)
```

Only 3 variables are used in this pruned tree regression. Those 3 variables are CheckingAccountStatus.none,  Duration, and SavingsAccountBonds.lt.100. All these 3 variables are key credit information. The pruned tree regression model is simple.


###3.	Generate the confusion matrix of predictions (actual “Bad” and actual “Good” versus predicted “Bad” and predicted “Good”) using this pruned tree in the training sample. 
###a.	How many interactions do you see? 

```{r}
table.train<-table(train=train[,10],prediction=predict(x2,type="class"));table.train
```

There are 4 interactions in this table. In train dataset using fitted values in tree regression model, 424 Good Class is correctly predicted and 105 bad Class is correctly predicted. The overall accuracy is (105+424)/(105+105+66+424) = 75.57%. The misclassification rate is (105+66)/(105+105+66+424)  = 24.43%. 


```{r}
round(prop.table(table.train,1),2)
```

There are 4 interactions in this table. In train dataset, 87% good class is correctly predicted and 50% bad class are correctly predicted. The prediction performance of this method is very good for good credit class. The prediction of bad credit class is not as good as good credit class. 


```{r}
round(prop.table(table.train,2),2)
```

There are 4 interactions in this table. In predictions, 80% of prediction is correct in good credit class and 61% of prediction is correct in bad credit class.

###b.	Can you interpret the tree? Do you like it? Comment

Yes. I can interpret the tree. In pruned tree, the first parameter used to split the tree is checkingAccountStatus.none. If the client's checkingAccountStatus.none>0.5, then they goes to good Credit class. Otherwise, the clients classification continue with other variable, Duration. And so on to variable savingsAccountBonds.It.100. The tree regression is generated. 4 interactions are generated. 

I like the tree regression a lot. It reduces model complexity when there are a lots of categarical variables and some numerical variables as in this dataset. The first complete tree is built on the based of the dataset and cp value corresponding to the lowest cross-validation error is identified. The cp value corresponding to the lowest xerror is used to generate a pruned tree. The pruned tree regression uses only 3 variables to classify the credit class. This analysis uses very few variables and predict the credit class straight forward.

###4.	Perform Holdout validation testing.
###a.	Generate the Confusion matrix in the holdout sample using the tree grown in the training data set.

The table of prediction is as below.

```{r}
table.holdout<-table(holdout=holdout[,10],prediction=predict(x2,newdata=holdout,type="class")); table.holdout
```

In holdout dataset using tree regression model, 177 Good Class is correctly predicted and 32 bad Class is correctly predicted. The overall accuracy is (32+177)/(32+58+33+177) = 69.67%. The misclassification rate is (58+33)/(32+58+33+177) = 30.33%. This ratio is close to the overall accuracy 75.57% and the misclassification rate 24.43% in train dataset.


```{r}
round(prop.table(table(holdout=holdout[,10],prediction=predict(x2,newdata=holdout,type="class")),1),2)  
```

84% good class in holdout is corretly predicted and 36% bad class in holdout dataset are correctly predicted. In comparision with train dataset, 87% good class is corretly predicted and 50% bad class are correctly predicted. The model performance of good class is very similar between train dataset and holdout dataset. The model performance of bad class is a little different between train dataset and holdout dataset.


```{r}
round(prop.table(table(holdout=holdout[,10],prediction=predict(x2,newdata=holdout,type="class")),2),2)
```

In predictions, 75% of prediction is correct in good credit class and 49% of prediction is correct in bad credit class.

###5.	Summarize your results.

In this part of analysis, classification tree regression is performed on GermanCredit dataset. The first tree classification model is a complete tree regression using the Class as output and other variables as independent variables in train dataset. The cp value corresponding to lowest cross-validation error (xerror) was selected at 2 splits. The cp value at the lowest xerror is used to generate a reduced size (pruned) tree in the training data set. The pruned tree uses three independent variables to classify the Credit Class. Using this pruned tree model, the overall accuracy in train dataset is 75.57% and the misclassification rate in train dataset is 24.43%. The overall accuracy of holdout is  69.67% and the misclassification rate of is 30.33%. These ratios in holdout are close to rates in train dataset. The performance of this model is good and stable. This analysis use 3 variables in the final model and predict the credit class straight forward.

###6.	SAVE your predictions of tree models – both for training and holdout data sets. You will need them later.

The predictions of tree regression are saved as below.

```{r}
train$pred <- predict(x2,type="class")
holdout$pred<-predict(x2,newdata=holdout,type="class")
tree.prediction<-c(predict(x2,type="class"),predict(x2,newdata=holdout,type="class"))
write.csv(tree.prediction,file="tree.prediction")
write.csv(train,file="tain.pred.tree")
write.csv(holdout,file="holdout.pred.tree")
```


###7.	Which has performed better for the GermanCredit data – tree or logistic regression?

In both train dataset and holdout dataset, the performance of logistic regression is slightly better than tree regression. 

The following table is the combined confusion tables in train dataset from logistic regression and tree regression.

```{r}
rbind(table1,table.train)
```

The following table is the combined confusion tables in holdout from logistic regression and tree regression.

```{r}
rbind(table2,table.holdout)
```


In train dataset, the overall accuracy of logistic regression is 80.14% and misclassification rate of logistic regression is 19.86%. These ratios are close to the overall accuracy of tree regression 75.57% and the misclassification rate of tree regression 24.43% 

In holdout dataset, the overall accuracy of logistic regression is 74% and the misclassification rate of logistic regression is 26%. These values are close to the accuracy of tree regression 69.67% and the misclassification rate 30.33%.

The model performance of logistic regression model in part 1 is better than tree model in part 2. The final logistic model uses 30 variables while the final pruned tree model uses three variables. The pruned tree model uses fewer variables than the logistic model.


